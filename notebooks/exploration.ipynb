{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8899ced5",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7098c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('../data/raw/name_pairs.csv')\n",
    "\n",
    "# Display first few rows\n",
    "print(\"First few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "print(f\"\\nMissing values:\\n{df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce2622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "print(\"Class distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nClass balance ratio: {df['label'].sum() / len(df):.2%} similar\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='label', data=df, palette=['lightcoral', 'lightgreen'])\n",
    "plt.xlabel('Label (0: Not Similar, 1: Similar)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759503a0",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54de07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Normalize: lowercase, remove punctuation, normalize spaces.\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "df['name_1_clean'] = df['name_1'].apply(clean_text)\n",
    "df['name_2_clean'] = df['name_2'].apply(clean_text)\n",
    "\n",
    "# Show examples\n",
    "print(\"Cleaning examples:\")\n",
    "for idx in range(5):\n",
    "    print(f\"  {df.iloc[idx]['name_1']} -> {df.iloc[idx]['name_1_clean']}\")\n",
    "    print(f\"  {df.iloc[idx]['name_2']} -> {df.iloc[idx]['name_2_clean']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eeaf64",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85570fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create vectorizer for character n-grams\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=(2, 3),\n",
    "    max_features=500,\n",
    "    lowercase=False  # Already cleaned\n",
    ")\n",
    "\n",
    "# Fit on all names\n",
    "all_names = np.concatenate([df['name_1_clean'].values, df['name_2_clean'].values])\n",
    "vectorizer.fit(all_names)\n",
    "\n",
    "# Transform\n",
    "vec1 = vectorizer.transform(df['name_1_clean'])\n",
    "vec2 = vectorizer.transform(df['name_2_clean'])\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarities = np.array([cosine_similarity(vec1[i], vec2[i])[0, 0] for i in range(len(df))])\n",
    "\n",
    "print(f\"Similarity score range: [{similarities.min():.3f}, {similarities.max():.3f}]\")\n",
    "print(f\"Mean similarity: {similarities.mean():.3f}\")\n",
    "print(f\"\\nFeature matrix shape: {similarities.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9613d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize similarity distribution by class\n",
    "df['similarity'] = similarities\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df[df['label'] == 0]['similarity'], bins=15, alpha=0.6, label='Not Similar', color='lightcoral')\n",
    "plt.hist(df[df['label'] == 1]['similarity'], bins=15, alpha=0.6, label='Similar', color='lightgreen')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Similarity Distribution by Class')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x='label', y='similarity', data=df, palette=['lightcoral', 'lightgreen'])\n",
    "plt.xlabel('Label (0: Not Similar, 1: Similar)')\n",
    "plt.ylabel('Similarity Score')\n",
    "plt.title('Similarity by Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd66245",
   "metadata": {},
   "source": [
    "## 4. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932c8903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare features and labels\n",
    "X = similarities.reshape(-1, 1)\n",
    "y = df['label'].values\n",
    "\n",
    "# Train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"\\nTrain label distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Test label distribution: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1b869a",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5779db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute class weights for imbalance\n",
    "classes = np.unique(y_train)\n",
    "weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "class_weight = {c: w for c, w in zip(classes, weights)}\n",
    "\n",
    "print(f\"Class weights: {class_weight}\")\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression(\n",
    "    class_weight=class_weight,\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fbd417",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc41c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"=== Model Evaluation ===\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall:    {recall:.3f}\")\n",
    "print(f\"F1 Score:  {f1:.3f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nDetailed Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Not Similar', 'Similar'], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f821e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Not Similar', 'Similar'],\n",
    "            yticklabels=['Not Similar', 'Similar'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bd5e22",
   "metadata": {},
   "source": [
    "## 7. Inference and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20f1b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_similarity(name_1, name_2):\n",
    "    \"\"\"Predict similarity between two names.\"\"\"\n",
    "    # Clean\n",
    "    name_1_clean = clean_text(name_1)\n",
    "    name_2_clean = clean_text(name_2)\n",
    "    \n",
    "    # Vectorize\n",
    "    v1 = vectorizer.transform([name_1_clean])\n",
    "    v2 = vectorizer.transform([name_2_clean])\n",
    "    \n",
    "    # Similarity\n",
    "    score = cosine_similarity(v1, v2)[0, 0]\n",
    "    X_pred = np.array([[score]])\n",
    "    \n",
    "    # Predict\n",
    "    decision = bool(model.predict(X_pred)[0])\n",
    "    confidence = model.predict_proba(X_pred)[0][1]\n",
    "    \n",
    "    return score, decision, confidence\n",
    "\n",
    "# Test examples\n",
    "test_pairs = [\n",
    "    ('ApplePay', 'Apple Pay'),\n",
    "    ('Google', 'Alphabet'),\n",
    "    ('Nike', 'Niike'),\n",
    "]\n",
    "\n",
    "print(\"=== Prediction Examples ===\")\n",
    "for name_1, name_2 in test_pairs:\n",
    "    score, decision, confidence = predict_similarity(name_1, name_2)\n",
    "    print(f\"\\n{name_1} vs {name_2}\")\n",
    "    print(f\"  Similarity: {score:.3f}\")\n",
    "    print(f\"  Decision: {'Similar' if decision else 'Not Similar'}\")\n",
    "    print(f\"  Confidence: {confidence:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
